---
title: "The Effect of Fictional Reappraisal on Subjective Ratings Toward Images"
shorttitle: "FicitionEro"
author:
  - name: Dominique Makowski
    corresponding: true
    orcid: 0000-0001-5375-9967
    email: D.Makowski@sussex.ac.uk
    url: https://realitybending.github.io/
    roles:
      - Project administration
      - Data curation
      - Formal Analysis
      - Investigation
      - Visualization
      - Writing – original draft
      - Writing – review & editing
    affiliations:
      - ref: "id1"
      - name: "University of Sussex"
        department:  "Sussex Centre for Consciousness Science"
  - name: Ana Neves
    orcid: 0009-0006-0020-7599
    roles:
      - Data curation
      - Formal Analysis
      - Investigation
      - Visualization
      - Writing – original draft
      - Writing – review & editing
    affiliations:
      - id: "id1"
        name: "University of Sussex"
        department: "School of Psychology"
# author-note:
#   status-changes: 
#     affiliation-change: null
#     deceased: null
#   disclosures:
#     study-registration: null
#     data-sharing: null
#     related-report: null
#     conflict-of-interest: null
#     financial-support: null
#     gratitude: null
#     authorship-agreements: null
    #   # - Project administration
    #   # - Data curation
    #   # - Formal Analysis
    #   # - Investigation
    #   # - Visualization
    #   # - Writing – original draft
    #   # - Writing – review & editing
    # Roles are optional. 
    # conceptualization, data curation, formal Analysis, funding acquisition, investigation, 
    # methodology, project administration, resources, software, supervision, validation, 
    # visualization, writing, editing
    #   - Conceptualization
    #   - Data curation
    #   - formal Analysis
    #   - Funding acquisition
    #   - Investigation
    #   - Methodology
    #   - Project administration
    #   - Resources
    #   - Software
    #   - Supervision
    #   - Validation
    #   - Visualization
    #   - Writing – original draft
author-note:
  disclosures:
    gratitude: |
      ::: {.callout-note icon=false appearance="simple"}
      :::
      This preprint is a non-peer-reviewed work from the [**Reality Bending Lab**](https://realitybending.github.io/).
      ![](https://realitybending.github.io/media/ReBeL_LogoOnly_hu11484441381606756729.png){width=20% fig-align="center"}
abstract: |
  Blabla the abstract blabla.
keywords: [keyword1, keyword2, keyword3]
floatsintext: true
numbered-lines: true
bibliography: bibliography.bib
suppress-title-page: false
mask: false
# Language options. See https://quarto.org/docs/authoring/language.html
lang: en
language:
  citation-last-author-separator: "and"
  citation-masked-author: "Masked Citation"
  citation-masked-date: "n.d."
  citation-masked-title: "Masked Title"
  title-block-author-note: "Author Note"
  title-block-correspondence-note: "Correspondence concerning this article should be addressed to"
  title-block-role-introduction: "Author roles were classified using the Contributor Role Taxonomy (CRediT; https://credit.niso.org/) as follows:"
  references-meta-analysis:
format:
  apaquarto-pdf:
    # Can be jou (journal), man (manuscript), stu (student), or doc (document)
    documentmode: jou
    include-in-header:
      - text: |
          \usepackage{lscape}
  apaquarto-docx: default
  apaquarto-html: default
editor: 
  markdown: 
    wrap: sentence
editor_options: 
  chunk_output_type: console
---


```{r}
#| label: setup
#| include: false
library(conflicted)
library(tidyverse)
library(flextable)
library(ftExtra)
library(officer)
library(knitr)

conflicts_prefer(dplyr::filter, .quiet = TRUE)
conflicts_prefer(flextable::separate_header, .quiet = TRUE)
```

```{r, echo = FALSE, warning=FALSE, message=FALSE}
# options and parameters
options(digits = 3)

knitr::opts_chunk$set(
    collapse = TRUE,
    dpi = 450,
    fig.width = see::golden_ratio(9),
    fig.height = 9,
    fig.path = "images/"
)

cache <- TRUE
```

<!-- AI and Reality distinction
  - rise of AI
  - societal consequences
  - indistinguishable nature of artificial vs real content -->

<!-- Duffy et al., 2024:  when image sare controlled for types of artifacts participants show no ability in distinguishing real vs fake faces. Altough, they did rate faces with higher intensity smiles as more real. -->

Recent advances in artificial intelligence (AI) have introduced new challenges for human cognition, particularly regarding the ability to distinguish between authentic and fabricated experiences [@miller2023ai]. 

The consequences of such perceptual uncertainty are considerable, with misinformation representing one of the most pressing societal consequences [@kreps2022all].
For instance, deepfakes (face-swapping technologies that enable the creation of realistic fake images and videos) have already been employed to fabricate convincing political speeches that appear genuine [@meskys2020regulating].
When deployed in electoral contexts, such content has the potential to distort public opinion and undermine democratic processes by rendering the truth unclear [@graber2021artificial]. 
Yet political disinformation is only one domain in which synthetic media exerts influence. 
The increased accessibility of generative technologies means that manipulated or entirely artificial content now permeates social media, entertainment, immersive environments, and interpersonal communication [@nightingale2022ai].  
In these settings, the boundaries between the “real” and the “artificial” are becoming progressively less discernible, raising fundamental questions about how individuals perceive and evaluate reality.

<!-- Reality Perception
  - ambiguous stimuli
  - perceptual cues, emotional influence, reality monitoring
  - AI getting better at producing realistic content -->


A key challenge lies in the prevalence of ambiguous stimuli, that is images, texts, videos, or environments whose authenticity cannot be easily established. 
While artificial stimuli once carried perceptual markers that made their inauthenticity easier to spot, such as distortions in early computer-generated imagery [@corvi2023detection; @mcdonnell2010face], these limitations are rapidly disappearing. 
In the domain of face generation especially, current algorithms now produce synthetic images that are virtually indistinguishable from genuine photographs [@moshel2022you; @nightingale2022ai]. 
As these technologies approach perfect realism, the study of reality perception becomes not only a theoretical concern but also a practical imperative, with direct implications for information security, social trust, and the ethical deployment of AI.

<!-- Affective Responses and Reality Judgments
  - Link perception of realism with emotional engagement: arousal, valence, enticement.
  - Mention theories such as Affective Reality Theory (Makowski, 2023) if relevant, or more general emotion–cognition links.
  - Emotions may not just follow perception, but also shape how we judge what is real -PP. -->
  


<!-- Sexual Stimuli and Pornography Consumption
  - Motivation behind inclusion of sexual/non-sexual content (e.g., sexual images are affectively intense, highly relevant for arousal/enticement measures).
  - Bring in literature on pornography consumption and attitudes toward AI in intimate contexts (questionnaires)
  - Highlight that this domain is both theoretically interesting (strong affective reactions) and socially relevant (AI-generated sexual content already exists, raising ethical debates). -->


# Study 1

## Methods

**Participants**

The initial sample comprised 1,067 participants recruited via multiple channels, including Prolific\textcopyright, Sona Systems, social media platforms, university classrooms, and snowball sampling. This strategy yielded a heterogeneous pool of both incentivised and non-incentivised individuals, including students and members of the general population from England, France, Italy, Colombia, and Spain.

To ensure data quality, several exclusion criteria were applied. Participants were removed if they (a) showed no variation in arousal ratings across trials (N = 8), (b) displayed a negative correlation between arousal and enticement alongside lower arousal ratings for erotic compared to neutral stimuli, suggesting a possible misunderstanding of scale direction (N = 4), or (c) self-identified with a gender or sexual orientation incompatible with the aims of the analysis (N = 350). For the latter, only self-reported heterosexual individuals were retained.

The final sample consisted of 705 participants (Mean age = 30.2 years $\pm$ 11.8; 35.7% female). Participants were primarily from the United Kingdom (28.23%), Italy (18.72%), the United States (14.33%), and Colombia (11.06%), with the remaining 27.66% distributed across other countries.
Ethical approval for this study was obtained from the School of Psychology Ethics Committee at the University of Sussex (ER/MHHE20/1).


### Materials

All written materials in this study were translated into the participants’ native languages: English, Italian, French, and Spanish. 

#### Questionnaires

The Beliefs about Artificial Image Technology (BAIT) scale assesses general attitudes toward artificial intelligence (AI) and beliefs about AI-generated media. It includes six items adapted from the General Attitudes towards Artificial Intelligence Scale [GAAIS, @schepman2020initial; @schepman2023general], comprising three positively valenced (e.g., “Artificial Intelligence is exciting”) and three negatively valenced items (e.g., “Artificial Intelligence might take control of people”). In addition, several items were developed to evaluate beliefs about computer-generated imagery, such as “Current Artificial Intelligence algorithms can generate very realistic images” and “Images of faces or people generated by Artificial Intelligence always contain errors and artifacts.” All items were rated on a continuous scale from strongly disagree (0) to strongly agree (1). One item was included to assess self-reported AI knowledge, with anchors ranging from Not at all (0) to Expert (6).

The Consumption of Pornography Scale – General [COPS, @hatch2023consumption] is a 34-item measure assessing pornography use across multiple dimensions, including frequency, duration and recency of sexual activity. Participants reported how often they had viewed pornography in the past 30 days (e.g., not at all, once or twice, weekly, daily, multiple times per day) and the typical duration of viewing sessions (less than 5 minutes to 46+ minutes). An additional item assessed the recency of any sexual activity, with response options ranging from within the past 24 hours to more than a year ago.

#### Affective Measures

#### Arousal

Subjective sexual arousal was assessed following each image with the question, “How much did you feel sexually aroused?” Responses were recorded on a continuous scale from Not at all (0) to Very much (1).

#### Enticement

Perceived enticement was measured after each image using the question, “How enticing would you rate this image to be?” with the same scale ranging from Not at all (0) to Very much (1).

#### Valence

Emotional valence was evaluated by asking, “The feeling evoked by the image was...” rated on a scale from Unpleasant (-1) to Pleasant (1).

#### Realism

In a final stage of the experiment, each image was shown again, and participants rated its perceived realism with the question, “How realistic was this image?” using a continuous scale anchored at AI-generated (0) and Photograph (1).

#### Feedback



**Procedure**

```{r}
#| warning: false
#| label: "fig-paradigm1"
#| apa-twocolumn: true  # A Figure Spanning Two Columns When in Journal Mode
#| out-width: "100%"
#| fig-cap: "Paradigm 2"


knitr::include_graphics("images/Paradigm1.png")
```

The study was conducted in line with the born-open principle [@de2024datapipe], ensuring transparency and reproducibility at every stage. The experiment was implemented entirely in jsPsych  [@de2015jspsych], with the full code hosted publicly on GitHub, which also served as the platform for running the online study. Raw data were automatically stored in a private Open Science Framework (OSF) repository. Anonymized data, together with all preprocessing and analysis scripts, will be openly released on GitHub to facilitate complete reproducibility.
Participants first provided informed consent before completing a short demographic questionnaire covering gender, age, ethnicity, country of residence, education, and English proficiency. Optional questions on birth control use were also included. They then proceeded to the experimental tasks.

<!-- EYE-tracking data? -->

In the first phase, participants were told that the study aimed to validate a new image-generation algorithm. They were informed that they would see images allegedly produced by this algorithm intermixed with real photographs, each preceded by a cue indicating whether the upcoming image was of an “AI-generated” or “Photograph” origin. Their task was to rate each image on arousal, enticement, and valence.
Each participant viewed, in a randomised order, 60 images in total: 40 erotic images (20 male and 20 female) from the Erotic subset of the Nencki Affective Picture System [NAPS ERO, @wierzba2015erotic], and 20 additional images (10 neutral, 10 positively arousing) from the original NAPS database [@marchewka2014nencki]. Each trial followed a fixed timing sequence: a fixation cross (750 ms), a color-coded textual cue (1,250 ms), another fixation cross (500 ms), then the image (2,500 ms). Cues were presented in red, green, or blue, with colors randomly assigned across participants.

Following each image, participants rated their emotional response using three continuous sliders assessing sexual arousal, enticement, and valence. This phase was self-paced, with responses required before continuing.
After completing the image-rating phase, participants filled out two self-report questionnaires: first the BAIT scale, followed by the COPS questionnaire.

In the final phase, participants viewed the same 60 images, presented in a new randomised order. Each was preceded by a 500 ms fixation cross and displayed for 1,000 ms. This time, participants rated each image on perceived realism—how photographic or lifelike it appeared.

At the end of the experiment, participants completed a feedback form. They were asked whether they could distinguish AI-generated from real images, whether AI images appeared more or less arousing, whether cue labels seemed accurate or reversed, and whether specific images stood out as particularly arousing or unarousing.
Finally, participants were debriefed on the true purpose of the study: to examine how image labels (AI-generated vs. real photograph) influence emotional responses. Importantly, they were informed that all images were real photographs, and that the “AI-generated” label was used solely to test the effect of belief on affective reactions. A shareable link to the experiment was also provided.


**Data Analysis**



# Study 2

## Methods

### Participants

The initial sample comprised 279 participants recruited via Prolific\textcopyright. Inclusion criteria required participants to be native English speakers or residents of countries with high levels of English proficiency. Participant exclusions were applied as follows: five participants were removed for showing no variability in arousal ratings (i.e., they did not move the response scales). An additional five participants were excluded for completing the study on a mobile device. One participant was excluded due to displaying negative correlations between arousal and both enticement and valence. Furthermore, five participants who self-identified as neither female nor male, and two participants who reported a sexual orientation other than heterosexual, homosexual, or bisexual, were excluded from further analyses. Finally, one participant was removed because the stimuli presented were not relevant to their gender and sexual orientation.

The final sample consisted of 261 participants (Mean = 37.4 $\pm$ 12.7, 48.7% Female). 56.32% of participants were from the United Kingdom, 26.82% from South Africa, 10.34% from the United States, and the remaining 6.51% were from other countries. 

Ethical approval for this study was obtained from the School of Psychology Ethics Committee at the University of Sussex (ER/EB672/2).

### Materials

#### Questionnaires

The questionnaires used in Study 2 were largely the same as those in Study 1, with minor modifications. In the BAIT, two items, assessing beliefs that AI might take control of people and interest in using AI systems in daily life, were removed. Additionally, the wording was streamlined by replacing “Artificial Intelligence” with “AI” throughout the scale. In the COPS, the item assessing the typical duration of pornography viewing sessions was omitted, retaining only items measuring frequency of pornography viewing and recency of sexual activity.

#### Affective Measures

#### Arousal
Subjective sexual arousal was assessed following each image with the question, “How much did you feel sexually aroused?” Responses were recorded on 6-point Likert scale from Not at all (0) to Very much (6).

#### Enticement
Perceived enticement was measured after each image using the question, “How enticing would you rate this image to be?” with the same scale ranging from 6-point Likert scale from Not at all (0) to Very much (6). 

#### Valence
Emotional valence was evaluated by asking, “The feeling evoked by the image was...” rated on a scale from Unpleasant (0) to Pleasant (6).

#### Reality
In a final stage of the experiment, each image was shown again, and participants rated on the images authenticity with the question, “I think this face is...Indicate your confidence that the image is fake or real” using a continuous scale anchored at AI-generated (-3) and Photograph (3).

#### Feedback 

### Procedure
```{r}
#| warning: false
#| label: "fig-paradigm2"
#| apa-twocolumn: true  # A Figure Spanning Two Columns When in Journal Mode
#| out-width: "100%"
#| fig-cap: "Paradigm 2"

knitr::include_graphics("images/Paradigm2.png")
```

Consistent with Study 1, Study 2 was conducted in jsPsych following born-open principles [@de2024datapipe; @de2015jspsych].Participants first provided informed consent, being informed that they could withdraw at any time; however, once the experiment was completed, withdrawal was not possible because the data were anonymized prior to storage. They then completed the same demographic questions as in Study 1, with the exception of items regarding birth control use.

<!-- EYE-tracking data? -->

In the first phase, participants were informed that the researchers were collaborating with a young AI start-up based in Brighton, intended to enhance the believability of the study. Participants were told that they would view images generated by this algorithm intermixed with “real” photographs, each preceded by a label indicating whether the image was AI-generated or a photograph. They were asked to rate each image on sexual arousal, enticement, and valence. A total of 50 images were presented, drawn from two categories of the NAPS-ERO database (25 images of couples and 25 images of individuals). 

Images were assigned to be relevant to participants’ self-reported gender and sexual orientation; for example, male participants identifying as homosexual viewed male individuals and male couples. Each trial followed a fixed timing sequence: a fixation cross (500 ms), a color-coded textual cue displayed for 1,000 ms, a second fixation cross (500 ms), and then the image presented for 2,000 ms. Cue colors were the same as in Study 1 and were randomly assigned across trials. Participants identifying as other in gender or bisexual/other in sexual orientation were asked which type of images they preferred, with options including “Women (and heterosexual couples),” “Men (and heterosexual couples),” “Only women (and lesbian couples),” and “Only men (and gay couples).”

Midway through the 50 images, participants were provided with a break and instructed to continue when ready. At this point, they completed a brief feedback survey assessing their subjective impressions of the images and AI-generation labels. This survey asked whether certain images were particularly arousing, whether AI-generated images were more or less arousing than the photographs, and participants’ perceptions of the AI-generation algorithm. Specifically, they indicated whether differences between AI-generated and real images were obvious or subtle, whether they perceived inconsistencies or reversals in labeling (“Photograph” vs. “AI-Generated”), and whether they believed all images were either photos or AI-generated. If participants indicated that all images were real or AI-generated, they rated their confidence on a scale from “Not at all” to “Completely certain.” This feedback captured participants’ explicit beliefs and subjective reactions regarding both the content and labeling of the images.Following the first phase, participants completed the BAIT and COPS questionnaires.

In the second phase, participants were informed that some images had been intentionally mislabeled and were asked to judge whether each image was AI-generated or a real photograph, expressing their confidence at the extremes of the scale. The timing procedure in this phase was identical to Study 1. After each image, participants provided general feedback regarding their experience and any additional comments. Finally, participants were presented with a debrief page informing them that all images were, in fact, real photographs (see @fig-paradigm2). 

### Data Analysis 


# References

<!-- References will auto-populate in the refs div below -->

::: {#refs}
:::
